# -*- coding: utf-8 -*-
"""
Created on Sat Oct 11 11:13:03 2025

@author: Yong Jin Park
"""

from pathlib import Path
import geopandas as gpd
import folium
from shapely.geometry import LineString, Point, Polygon
import branca.colormap as cm

import pandas as pd
from datetime import datetime
from io import StringIO
from tqdm import tqdm
import geopandas as gpd
import folium
import matplotlib.pyplot as plt
import os
import numpy as np
import json 
from shapely.geometry import Polygon
from shapely.geometry import MultiPolygon
from shapely.geometry import box
from shapely.strtree import STRtree
import gc
from pathlib import Path
# import os
from glob import glob
import time
from line_profiler import profile
# from sqlalchemy import create_engine
import psycopg2
import logging, psutil

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

plt.rc('font', family='NanumGothic')

#%%
# ê¸°ì¤€ ê²½ë¡œ ë° ê²€ìƒ‰ íŒ¨í„´ ì„¤ì •
BASE_PATH = Path.cwd()
# MODIFIED: '251011' í´ë”ë¥¼ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ë„ë¡ íŒ¨í„´ ë³µì›
# SEARCH_PATTERN = 
YEAR = 2025

# --- 2. ë§í¬(Links) ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---
# ì´ ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.
try:
    logging.info("ì›ë³¸ ë§í¬ Shapefileì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    # ì‚¬ìš©ìžì˜ í™˜ê²½ì— ë§žê²Œ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    links_shp_path = "/data1/DTG/2023_1/JB/JBLINK.shp"
    links = gpd.read_file(links_shp_path, engine='pyogrio', columns=['LINK_ID', "ROAD_RANK", 'geometry'])
    
    logging.info("ROAD_RANKê°€ '103'ì¸ ë§í¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.")
    links103_ids = links[links['ROAD_RANK'] == "103"]['LINK_ID'].tolist()
    links_filtered = links[links['LINK_ID'].isin(links103_ids)].copy()
    logging.info(f"ì´ {len(links_filtered)}ê°œì˜ '103' ë“±ê¸‰ ë§í¬ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    logging.error(f"ë§í¬ Shapefile ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    links_filtered = None


def aggregate_dtg_data(base_path, search_pattern, year, links_df):
    """
    ì§€ì •ëœ ê²½ë¡œì™€ íŒ¨í„´ì— ë§žëŠ” geoparquet íŒŒì¼ë“¤ì„ ì°¾ì•„ ì§‘ê³„í•˜ê³ ,
    íŒŒì¼ì´ ë°œê²¬ëœ ìœ„ì¹˜ì— ìµœì¢… ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if links_df is None or links_df.empty:
        logging.error("ìœ íš¨í•œ ë§í¬ ë°ì´í„°ê°€ ì—†ì–´ ì§‘ê³„ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    start_time = time.time()
    logging.info("ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ì„ ì‹œìž‘í•©ë‹ˆë‹¤.")

    # --- 3. ì¤‘ê°„ ì§‘ê³„ íŒŒì¼ ê²€ìƒ‰ ---
    logging.info(f"'{base_path}' ê²½ë¡œì—ì„œ '{search_pattern}' íŒ¨í„´ìœ¼ë¡œ íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    intermediate_files = list(base_path.glob(search_pattern))
    
    # 'FINAL'ì´ í¬í•¨ëœ íŒŒì¼ ì œì™¸
    initial_file_count = len(intermediate_files)
    intermediate_files = [f for f in intermediate_files if "FINAL" not in f.name]
    final_file_count = len(intermediate_files)
    
    if not intermediate_files:
        logging.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìž‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
        
    logging.info(f"{initial_file_count}ê°œì˜ íŒŒì¼ì„ ì°¾ì•˜ê³ , 'FINAL' ì œì™¸ í›„ ì´ {final_file_count}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # MODIFIED: ì¶œë ¥ í´ë”ë¥¼ ê²€ìƒ‰ëœ íŒŒì¼ì˜ ìœ„ì¹˜ë¡œ ë™ì  ì„¤ì •
    output_folder = intermediate_files[0].parent
    logging.info(f"ê²°ê³¼ íŒŒì¼ì€ ì›ë³¸ íŒŒì¼ê³¼ ë™ì¼í•œ ìœ„ì¹˜ì¸ '{output_folder}'ì— ì €ìž¥ë©ë‹ˆë‹¤.")


    # --- 4. ë°ì´í„° ì§‘ê³„ ---
    aggregated_results = []
    
    for f in tqdm(intermediate_files, desc="Chunk íŒŒì¼ ì§‘ê³„ ì¤‘"):
        try:
            chunk_gdf = gpd.read_parquet(f)
            current_agg = chunk_gdf.groupby('LINK_ID')[['vehicle_count', 'VLM']].sum().reset_index()
            aggregated_results.append(current_agg)
        except Exception as e:
            logging.error(f"íŒŒì¼ '{f}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    if not aggregated_results:
        logging.warning("ì§‘ê³„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìž‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    logging.info("ëª¨ë“  chunk íŒŒì¼ì˜ ì§‘ê³„ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•© ë° ìµœì¢… ì§‘ê³„í•©ë‹ˆë‹¤.")
    final_agg_df = pd.concat(aggregated_results, ignore_index=True)
    final_agg_df = final_agg_df.groupby('LINK_ID')[['vehicle_count', 'VLM']].sum().reset_index()
    
    logging.info("ìµœì¢… ì§‘ê³„ ì™„ë£Œ. Geometry ì •ë³´ì™€ ë³‘í•©ì„ ì‹œìž‘í•©ë‹ˆë‹¤.")

    # --- 5. Geometry ì •ë³´ ê²°í•© ë° í›„ì²˜ë¦¬ ---
    links_geom = links_df[['LINK_ID', 'geometry']].drop_duplicates(subset=['LINK_ID'])
    final_merged_gdf = pd.merge(links_geom, final_agg_df, on='LINK_ID', how='left')

    final_merged_gdf['vehicle_count'] = final_merged_gdf['vehicle_count'].fillna(0).astype(int)
    final_merged_gdf['VLM'] = final_merged_gdf['VLM'].fillna(0).astype(int)
    
    final_merged_gdf = final_merged_gdf[(final_merged_gdf['vehicle_count'] > 0) | (final_merged_gdf['VLM'] > 0)]
    
    # --- 6. ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ìž¥ ---
    if final_merged_gdf.empty:
        logging.warning("ìµœì¢… ê²°ê³¼ ë°ì´í„°ê°€ ë¹„ì–´ìžˆì–´ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        # ë™ì ìœ¼ë¡œ ì„¤ì •ëœ output_folder ì‚¬ìš©
        final_output_path = output_folder / f"dtgsum_link_{year}_FINAL_RESULT.geoparquet"
        final_merged_gdf.to_parquet(final_output_path, engine='pyarrow', compression='snappy', index=False)
        logging.info(f"ì„±ê³µì ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {final_output_path}")

    end_time = time.time()
    logging.info(f"ì´ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")

if __name__ == "__main__":
    # `links_filtered`ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆì„ ë•Œë§Œ í•¨ìˆ˜ ì‹¤í–‰
    if links_filtered is not None:
        aggregate_dtg_data(BASE_PATH, "**/251011/dtgsum_link_2025_*.geoparquet", YEAR, links_filtered)
        aggregate_dtg_data(BASE_PATH, "**/251012/dtgsum_link_2025_*.geoparquet", YEAR, links_filtered)

#%%
# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •
#í•´ë‹¹ íŒŒì´ì¬ íŒŒì¼ì´ ìžˆëŠ” í•˜ìœ„ í´ë”ë¥¼ ëª¨ë‘ ì°¾ì•„ì„œ "dtgsum_link_2025_FINAL_RESULT.geoparquet" íŒŒì¼ì„ ì°¾ì•„ì„œ ëª¨ë‘ í•©ì¹˜ëŠ” ì½”ë“œë¡œ ìˆ˜ì •í•´ì•¼í•¨
# input_path = Path(r"????")
# input_path = Path(os.getcwd())
input_path = Path.cwd()
# intermediate_files = glob(os.path.join(input_path, f"dtgsum_link_2025_*.geoparquet"))
# intermediate_files = glob(os.path.join(input_path, f"251011/dtgsum_link_2025_*.geoparquet"))
search_pattern = "dps_RES0012522/output_JB/251004/251011/dtgsum_link_2025_FINAL_RESULT.geoparquet"

intermediate_files = list(input_path.glob(search_pattern))
print(f"'{search_pattern}' íŒ¨í„´ìœ¼ë¡œ ì´ {len(intermediate_files)}ê°œì˜ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

print(intermediate_files)
#dtgsum_link_2025_051_002
# intermediate_files = glob(os.path.join(input_path, f"dtgsum_link_2025_FINAL_RESULT.geoparquet"))

if not intermediate_files:
    raise FileNotFoundError(f"ì§€ì •ëœ ê²½ë¡œì— GeoParquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_path}")

# --- íš¨ìœ¨ì ìœ¼ë¡œ ê°œì„ ëœ ë°ì´í„° ì§‘ê³„ ë¶€ë¶„ ---
agg_results = []
# geometry ì •ë³´ë¥¼ ë‹´ì•„ë‘˜ GeoDataFrame (ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ì´ˆê¸°í™”)
geometry_gdf = gpd.read_parquet(intermediate_files[0])[['LINK_ID', 'geometry']].drop_duplicates(subset=['LINK_ID']).set_index('LINK_ID')

for f in tqdm(intermediate_files, desc="Aggregating chunk files"):
    chunk_df = pd.read_parquet(f, columns=['LINK_ID', 'vehicle_count', 'VLM'])
    current_agg = chunk_df.groupby('LINK_ID')[['vehicle_count', 'VLM']].sum()
    agg_results.append(current_agg)

logging.info("ëª¨ë“  íŒŒì¼ ì§‘ê³„ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")
# ëª¨ë“  ì§‘ê³„ ê²°ê³¼ë¥¼ í•œ ë²ˆì— í•©ì‚°
final_agg_df = pd.concat(agg_results).groupby('LINK_ID').sum().reset_index()

logging.info("ì§‘ê³„ ì™„ë£Œ. Geometry ì •ë³´ì™€ ë³‘í•©ì„ ì‹œìž‘í•©ë‹ˆë‹¤.")
# ì§‘ê³„ ê²°ê³¼ì™€ Geometry ì •ë³´ë¥¼ LINK_ID ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
# ì´ì œ final_gdfëŠ” GeoDataFrameì´ ë©ë‹ˆë‹¤.
final_gdf = gpd.GeoDataFrame(final_agg_df.merge(geometry_gdf, on='LINK_ID', how='inner'))

links = gpd.read_file(r"/data1/DTG/2023_1/JB/JBLINK.shp", engine='pyogrio',columns=['LINK_ID',"ROAD_RANK",'geometry'])
# links = gpd.read_file(r"D:\NIPA_GIT\TAMSPython\TAMSPython\JB\JBLINK.shp", engine='pyogrio',columns=['LINK_ID',"ROAD_RANK",'geometry'])
links103 = links[links['ROAD_RANK']=="103"]['LINK_ID'].tolist()
links103 = links[links['LINK_ID'].isin(links103)]

# ìµœì¢… ì§‘ê³„ ê²°ê³¼ì— ë§í¬ì˜ geometry ì •ë³´ ê²°í•©
links_geom = links103[['LINK_ID', 'geometry']].drop_duplicates(subset=['LINK_ID'])
final_merged_gdf = pd.merge(links_geom, final_gdf, on='LINK_ID', how='left')

# ì§‘ê³„ë˜ì§€ ì•Šì€ ë§í¬ëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
final_merged_gdf['vehicle_count'] = final_merged_gdf['vehicle_count'].fillna(0).astype(int)
final_merged_gdf['VLM'] = final_merged_gdf['VLM'].fillna(0).astype(int)
print(final_merged_gdf.columns)

del final_merged_gdf['geometry_y']
final_merged_gdf = final_merged_gdf.rename(columns={'geometry_x': 'geometry'})
print(final_merged_gdf.columns)
final_merged_gdf.set_geometry("geometry")

final_merged_gdf = final_merged_gdf.set_crs(epsg=32652, allow_override=True)
# --- ì½”ë“œ ì‹¤í–‰ ë¶€ë¶„ ---
# 'ratio' ì»¬ëŸ¼ ê³„ì‚°
final_merged_gdf['ratio'] = np.where(
    final_merged_gdf['VLM'] == 0, 
    0, 
    (final_merged_gdf['vehicle_count'] / final_merged_gdf['VLM']) * 100
)

final_merged_gdf['ratio'] = final_merged_gdf['ratio'].round(1)
print(final_merged_gdf.info())

print(final_merged_gdf.head())
print(final_merged_gdf.tail())

#%%
output_path_excel = os.path.join(input_path, "final_merged_gdf2ì‹œê°„ì´ìƒ.xlsx")
final_merged_gdf.to_excel(output_path_excel, index=False)

output_path_gdf = os.path.join(input_path, "final_merged_gdf2ì‹œê°„ì´ìƒ.geoparquet")
final_merged_gdf.to_parquet(output_path_gdf, engine='pyarrow', compression='snappy', index=False)

#%%
# --- ì§€ë„ ìƒì„± í•¨ìˆ˜ (ìˆ˜ì • ì—†ìŒ, ê·¸ëŒ€ë¡œ ì‚¬ìš©) ---
def create_map(gdf, column_name, output_filename):
    """
    ì£¼ì–´ì§„ GeoDataFrameê³¼ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ Folium ì§€ë„ë¥¼ ìƒì„±í•˜ê³  HTML íŒŒì¼ë¡œ ì €ìž¥í•©ë‹ˆë‹¤.
    (í•¨ìˆ˜ ë‚´ìš©ì€ ì›ë³¸ê³¼ ë™ì¼)
    """
    try:
        print(f"'{column_name}'ì— ëŒ€í•œ ì§€ë„ ìƒì„±ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")
        gdf_copy = gdf.copy()
        gdf_copy[column_name] = gdf_copy[column_name].fillna(0)

        if gdf_copy.crs is None:
            gdf_copy = gdf_copy.set_crs(epsg=32652, allow_override=True)

        gdf_copy = gdf_copy.to_crs(epsg=4326)
        center = ((gdf_copy.total_bounds[1] + gdf_copy.total_bounds[3]) / 2.0,
                  (gdf_copy.total_bounds[0] + gdf_copy.total_bounds[2]) / 2.0)
        
        m = folium.Map(location=center, zoom_start=11, control_scale=True)

        folium.TileLayer(
            tiles="https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}",
            attr="Esri",
            name="Esri Satellite",
            overlay=False,
            control=True
        ).add_to(m)
        
        valid_data = gdf_copy[gdf_copy[column_name] >= 0][column_name]
        vmin = valid_data.min() if not valid_data.empty else 0
        vmax = valid_data.max() if not valid_data.empty else 1

        colormap = cm.LinearColormap(colors=["black", "blue", "red"], vmin=vmin, vmax=vmax, caption=column_name)
        
        def style_fn(feature):
            value = feature['properties'].get(column_name, 0)
            try:
                value = float(value)
            except (ValueError, TypeError):
                value = 0.0
            
            if value < 0:
                return {'fillOpacity': 0, 'weight': 0}
            else:
                return {'color': colormap(value), 'weight': 3, 'opacity': 0.8}
        
        folium.GeoJson(
            gdf_copy,
            name=column_name,
            style_function=style_fn,
            tooltip=folium.features.GeoJsonTooltip(
                fields=["LINK_ID", column_name], 
                aliases=["LINK_ID:", f"{column_name}:"]
            ),
            overlay=True,
            control=True
        ).add_to(m)
        
        colormap.add_to(m)
        folium.LayerControl().add_to(m)
        
        m.save(output_filename)
        print(f"âœ… ì„±ê³µ! ì§€ë„ë¥¼ '{output_filename}' íŒŒì¼ë¡œ ì €ìž¥í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: '{column_name}' ì§€ë„ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({repr(e)})")


# ê° ì»¬ëŸ¼ì— ëŒ€í•œ ì§€ë„ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ (ì´ì œ GeoDataFrameì¸ final_gdfë¥¼ ì „ë‹¬)
create_map(final_merged_gdf, 'vehicle_count', 'map_2ì‹œê°„ì´ìƒì£¼í–‰ì°¨ëŸ‰ëŒ€ìˆ˜.html')
create_map(final_merged_gdf, 'VLM', 'map_2ì‹œê°„ì´ìƒì „ì²´êµí†µëŸ‰.html')
create_map(final_merged_gdf, 'ratio', 'map_2ì‹œê°„ì´ìƒratio.html')

print("\nðŸŽ‰ ëª¨ë“  ì§€ë„ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

#%%
# 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •
#í•´ë‹¹ íŒŒì´ì¬ íŒŒì¼ì´ ìžˆëŠ” í•˜ìœ„ í´ë”ë¥¼ ëª¨ë‘ ì°¾ì•„ì„œ "dtgsum_link_2025_FINAL_RESULT.geoparquet" íŒŒì¼ì„ ì°¾ì•„ì„œ ëª¨ë‘ í•©ì¹˜ëŠ” ì½”ë“œë¡œ ìˆ˜ì •í•´ì•¼í•¨
# input_path = Path(r"????")
# input_path = Path(os.getcwd())
input_path = Path.cwd()
# intermediate_files = glob(os.path.join(input_path, f"dtgsum_link_2025_*.geoparquet"))
# intermediate_files = glob(os.path.join(input_path, f"251011/dtgsum_link_2025_*.geoparquet"))
# search_pattern = "**/251012/dtgsum_link_2025_*.geoparquet"
search_pattern = "dps_RES0012522/output_JB/251004/251012/dtgsum_link_2025_FINAL_RESULT.geoparquet"

intermediate_files = list(input_path.glob(search_pattern))
print(f"'{search_pattern}' íŒ¨í„´ìœ¼ë¡œ ì´ {len(intermediate_files)}ê°œì˜ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

print(intermediate_files)
#dtgsum_link_2025_051_002
# intermediate_files = glob(os.path.join(input_path, f"dtgsum_link_2025_FINAL_RESULT.geoparquet"))

if not intermediate_files:
    raise FileNotFoundError(f"ì§€ì •ëœ ê²½ë¡œì— GeoParquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_path}")

# --- íš¨ìœ¨ì ìœ¼ë¡œ ê°œì„ ëœ ë°ì´í„° ì§‘ê³„ ë¶€ë¶„ ---
agg_results = []
# geometry ì •ë³´ë¥¼ ë‹´ì•„ë‘˜ GeoDataFrame (ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ì´ˆê¸°í™”)
geometry_gdf = gpd.read_parquet(intermediate_files[0])[['LINK_ID', 'geometry']].drop_duplicates(subset=['LINK_ID']).set_index('LINK_ID')

for f in tqdm(intermediate_files, desc="Aggregating chunk files"):
    chunk_df = pd.read_parquet(f, columns=['LINK_ID', 'vehicle_count', 'VLM'])
    current_agg = chunk_df.groupby('LINK_ID')[['vehicle_count', 'VLM']].sum()
    agg_results.append(current_agg)

logging.info("ëª¨ë“  íŒŒì¼ ì§‘ê³„ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...")
# ëª¨ë“  ì§‘ê³„ ê²°ê³¼ë¥¼ í•œ ë²ˆì— í•©ì‚°
final_agg_df = pd.concat(agg_results).groupby('LINK_ID').sum().reset_index()

logging.info("ì§‘ê³„ ì™„ë£Œ. Geometry ì •ë³´ì™€ ë³‘í•©ì„ ì‹œìž‘í•©ë‹ˆë‹¤.")
# ì§‘ê³„ ê²°ê³¼ì™€ Geometry ì •ë³´ë¥¼ LINK_ID ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
# ì´ì œ final_gdfëŠ” GeoDataFrameì´ ë©ë‹ˆë‹¤.
final_gdf = gpd.GeoDataFrame(final_agg_df.merge(geometry_gdf, on='LINK_ID', how='inner'))

links = gpd.read_file(r"/data1/DTG/2023_1/JB/JBLINK.shp", engine='pyogrio',columns=['LINK_ID',"ROAD_RANK",'geometry'])
# links = gpd.read_file(r"D:\NIPA_GIT\TAMSPython\TAMSPython\JB\JBLINK.shp", engine='pyogrio',columns=['LINK_ID',"ROAD_RANK",'geometry'])
links103 = links[links['ROAD_RANK']=="103"]['LINK_ID'].tolist()
links103 = links[links['LINK_ID'].isin(links103)]

# ìµœì¢… ì§‘ê³„ ê²°ê³¼ì— ë§í¬ì˜ geometry ì •ë³´ ê²°í•©
links_geom = links103[['LINK_ID', 'geometry']].drop_duplicates(subset=['LINK_ID'])
final_merged_gdf = pd.merge(links_geom, final_gdf, on='LINK_ID', how='left')

# ì§‘ê³„ë˜ì§€ ì•Šì€ ë§í¬ëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
final_merged_gdf['vehicle_count'] = final_merged_gdf['vehicle_count'].fillna(0).astype(int)
final_merged_gdf['VLM'] = final_merged_gdf['VLM'].fillna(0).astype(int)
print(final_merged_gdf.columns)

del final_merged_gdf['geometry_y']
final_merged_gdf = final_merged_gdf.rename(columns={'geometry_x': 'geometry'})
print(final_merged_gdf.columns)
final_merged_gdf.set_geometry("geometry")

final_merged_gdf = final_merged_gdf.set_crs(epsg=32652, allow_override=True)
# --- ì½”ë“œ ì‹¤í–‰ ë¶€ë¶„ ---
# 'ratio' ì»¬ëŸ¼ ê³„ì‚°
final_merged_gdf['ratio'] = np.where(
    final_merged_gdf['VLM'] == 0, 
    0, 
    (final_merged_gdf['vehicle_count'] / final_merged_gdf['VLM']) * 100
)

final_merged_gdf['ratio'] = final_merged_gdf['ratio'].round(1)
print(final_merged_gdf.info())

print(final_merged_gdf.head())
print(final_merged_gdf.tail())

#%%
output_path_excel = os.path.join(input_path, "final_merged_gdf2ì‹œê°„30ë¶„ì´ìƒ.xlsx")
final_merged_gdf.to_excel(output_path_excel, index=False)

output_path_gdf = os.path.join(input_path, "final_merged_gdf2ì‹œê°„30ë¶„ì´ìƒ.geoparquet")
final_merged_gdf.to_parquet(output_path_gdf, engine='pyarrow', compression='snappy', index=False)

#%%

# ê° ì»¬ëŸ¼ì— ëŒ€í•œ ì§€ë„ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ (ì´ì œ GeoDataFrameì¸ final_gdfë¥¼ ì „ë‹¬)
create_map(final_merged_gdf, 'vehicle_count', 'map_2ì‹œê°„30ë¶„ì´ìƒì£¼í–‰ì°¨ëŸ‰ëŒ€ìˆ˜.html')
create_map(final_merged_gdf, 'VLM', 'map_2ì‹œê°„30ë¶„ì´ìƒ_ì „ì²´êµí†µëŸ‰.html')
create_map(final_merged_gdf, 'ratio', 'map_2ì‹œê°„30ë¶„ì´ìƒ_ratio.html')

print("\nðŸŽ‰ ëª¨ë“  ì§€ë„ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
